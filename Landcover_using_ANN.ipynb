{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Landcover_using_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heromiya/GEE-LCM/blob/master/Landcover_using_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rectified-adam"
      ],
      "metadata": {
        "id": "wG3wG8JOOxes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLo1L6VV6qU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0IvUmXNXR_r"
      },
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS7jU7nPXugj"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import folium\n",
        "\n",
        "import gdal\n",
        "import osr\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E65NwWoEYUhW"
      },
      "source": [
        "def reduce_class_value(feat):\n",
        "  return feat.set('class', ee.Number(feat.get('class')).subtract(1))\n",
        "\n",
        "def cloudMask(img):\n",
        "  return img.updateMask(img.select('BQA').lt(64))\n",
        "\n",
        "def select_landsat(year):\n",
        "  if (year >= 2013):\n",
        "    return {\n",
        "        'bands': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7'],\n",
        "        'FCCbands': ['B5', 'B4', 'B3'],\n",
        "        'TextureBand': ['B5'],\n",
        "        'bandsClassify': ['B2', 'B3', 'B4', 'B5', 'B6', 'B7','B5_1'],\n",
        "        'Landsat': 'LANDSAT/LC08/C01/T1'\n",
        "    }\n",
        "\n",
        "  if (year >= 1999 and year <= 2002):\n",
        "    return {\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'],\n",
        "        'TextureBand': ['B4'],\n",
        "        'bandsClassify': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7','B4_1'],\n",
        "        'FCCbands': ['B4', 'B3', 'B2'],\n",
        "        'Landsat': 'LANDSAT/LE07/C01/T1'\n",
        "    }\n",
        "\n",
        "  if (year >= 1984 and year <= 1998 or year >=2003 and year <=2012):\n",
        "    return{\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'],\n",
        "        'bandsClassify': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7','B4_1'],\n",
        "        'FCCbands': ['B4', 'B3', 'B2'],\n",
        "        'TextureBand': ['B4'],\n",
        "        'Landsat': 'LANDSAT/LT05/C01/T1'\n",
        "    }\n",
        "  \n",
        "  if (year == 1983):\n",
        "    return{\n",
        "        'bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7'],\n",
        "        'bandsClassify': ['B1', 'B2', 'B3', 'B4', 'B5', 'B7','B4_1'],\n",
        "        'FCCbands': ['B4', 'B3', 'B2'],\n",
        "        'TextureBand': ['B4'],\n",
        "        'Landsat': 'LANDSAT/LT04/C01/T1'\n",
        "    }\n",
        "  \n",
        "  if (year >= 1979 and year <= 1982):\n",
        "    return {\n",
        "        'bands':         ['B4_median', 'B5_median', 'B6_median','B7_median'],\n",
        "        'bandsClassify': ['B4_median', 'B5_median', 'B6_median','B7_median','B6_median_1'],\n",
        "        'FCCbands':      ['B6_median', 'B5_median', 'B4_median'],\n",
        "        'TextureBand':   ['B6_median'],\n",
        "        'Landsat': 'LANDSAT/LM03/C01/T1'\n",
        "    }\n",
        "\n",
        "  if (year >= 1975 and year <= 1978):\n",
        "    return {\n",
        "        'bands':         ['B4_median', 'B5_median', 'B6_median','B7_median'],\n",
        "        'bandsClassify': ['B4_median', 'B5_median', 'B6_median','B7_median','B6_median_1'],\n",
        "        'FCCbands':      ['B6_median', 'B5_median', 'B4_median'],\n",
        "        'TextureBand':   ['B6_median'],\n",
        "        'Landsat': 'LANDSAT/LM02/C01/T2'\n",
        "    }\n",
        "\n",
        "  if (year >= 1972 and year <= 1974):\n",
        "    return {\n",
        "        'bands':         ['B4_median', 'B5_median', 'B6_median','B7_median'],\n",
        "        'bandsClassify': ['B4_median', 'B5_median', 'B6_median','B7_median','B6_median_1'],\n",
        "        'FCCbands':      ['B6_median', 'B5_median', 'B4_median'],\n",
        "        'TextureBand':   ['B6_median'],\n",
        "        'Landsat': 'LANDSAT/LM01/C01/T2'\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FtYc9uPaFFr"
      },
      "source": [
        "def landsat_product(landsat_params):\n",
        "  landsat_id = landsat_params['Landsat']\n",
        "  if(landsat_id == 'LANDSAT/LM01/C01/T2' or landsat_id == 'LANDSAT/LM02/C01/T2' or landsat_id == 'LANDSAT/LM03/C01/T1'):\n",
        "    image = ee.ImageCollection(landsat_id).filterDate(str(gtYearBegin) + '-01-01',str(gtYearEnd) + '-12-31').filter(ROI[roi]['doyFilterLandsat']).filterBounds(out_ext).filterMetadata('CLOUD_COVER_LAND', 'less_than', 20).map(cloudMask).reduce(ee.Reducer.median())\n",
        "    return {\n",
        "      'region': image.geometry().bounds().getInfo(),\n",
        "      'image': image,\n",
        "      'image_int': image.toByte()       \n",
        "    }\n",
        "  else:\n",
        "    image_col = ee.ImageCollection(landsat_id).filterDate(str(gtYearBegin) + '-01-01',str(gtYearEnd) + '-12-31').filter(ROI[roi]['doyFilterLandsat']).filterBounds(out_ext)\n",
        "    return {\n",
        "      'region': image_col.geometry().bounds().getInfo(),\n",
        "      'image': ee.Algorithms.Landsat.simpleComposite(image_col, 50, cloud, 40, True).select(landsat_params['bands']),\n",
        "      'image_int': ee.Algorithms.Landsat.simpleComposite(image_col, 50, cloud, 40, False).select(landsat_params['bands'])\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM6ZiXv9aHHS"
      },
      "source": [
        "def add_indices(params, product):\n",
        "  L = params['Landsat']\n",
        "  if(L =='LANDSAT/LC08/C01/T1'):\n",
        "    swir  = product['image'].select('B6')\n",
        "    nir   = product['image'].select('B5')\n",
        "    red   = product['image'].select('B4')\n",
        "    green = product['image'].select('B3')\n",
        "\n",
        "  if(L == 'LANDSAT/LE07/C01/T1'):\n",
        "    swir  = product['image'].select('B5')\n",
        "    nir   = product['image'].select('B4')\n",
        "    red   = product['image'].select('B3')\n",
        "    green = product['image'].select('B2')\n",
        "\n",
        "  if(L == 'LANDSAT/LM04/C01/T1' or L == 'LANDSAT/LT05/C01/T1'):\n",
        "    swir  = product['image'].select('B5')\n",
        "    nir   = product['image'].select('B4')\n",
        "    red   = product['image'].select('B3')\n",
        "    green = product['image'].select('B2')\n",
        "\n",
        "  ndvi  = nir.subtract(red).divide(nir.add(red))\n",
        "  ndbi  = swir.subtract(nir).divide(swir.add(nir))\n",
        "  bi    = ndbi.subtract(ndvi)\n",
        "  ndwi  = green.subtract(swir).divide(green.add(swir))\n",
        "\n",
        "  product['image'] = product['image'].addBands([ndvi, ndbi, bi, ndwi])\n",
        "  product['image'] = product['image'].rename(ee.List(params['bands'] + ['ndvi','ndbi','bi','ndwi']))\n",
        "  params['bandsClassify'].extend(['ndvi', 'ndbi', 'bi', 'ndwi'])\n",
        "\n",
        "def add_indices_mss(params, product):\n",
        "  nir = product['image'].select('B6_median')\n",
        "  red = product['image'].select('B5_median')\n",
        "  ndvi = nir.subtract(red).divide(nir.add(red))\n",
        "\n",
        "  product['image'] = product['image'].addBands(ndvi)\n",
        "  product['image'] = product['image'].rename(ee.List(params['bands'] + ['ndvi']))\n",
        "  params['bandsClassify'].append('ndvi')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFiq7qbnhv_y"
      },
      "source": [
        "##### User defined paramters ######\n",
        "\n",
        "ts = time.time()\n",
        "timestamp = int(ts)\n",
        "\n",
        "cloud = 30\n",
        "spatial_resolution = 30\n",
        "n_sample = 10000\n",
        "distance = 370000\n",
        "\n",
        "kernel_size_list = [1, 3, 5, 7, 9]\n",
        "\n",
        "base_distance = 5000\n",
        "patch_size = 256\n",
        "buffer_distance = base_distance * (round((spatial_resolution * 0.5 * patch_size)/base_distance) +1)\n",
        "\n",
        "USER_NAME = 'heromiya'\n",
        "GT_DATA = 'users/heromiya/gt-pt-170-52-2019-2021'\n",
        "\n",
        "ROI = [\n",
        "       {'cityName': 'Ethiopia-2-1_lower-left_res120_spring', 'LatMax': 11.0, 'LatMin': 10.0, 'LonMax': 37.85, 'LonMin': 36.5, 'yearBegin':2019, 'yearEnd':2021, 'doyFilter':ee.Filter.And(ee.Filter.greaterThanOrEquals('doy',  1), ee.Filter.lessThanOrEquals('doy',  366)), 'doyFilterLandsat': ee.Filter.dayOfYear(244,335)},\n",
        "]\n",
        "\n",
        "MY_DRIVE_PATH = '/content/drive/MyDrive'\n",
        "SHARED_DRIVE_PATH = '/content/drive/Shareddrives/Miyazaki Lab./' + USER_NAME + '/'\n",
        "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/demo'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd4ONLeBY09v"
      },
      "source": [
        "roi= ''\n",
        "out_ext = ''\n",
        "\n",
        "for roi in range(0, len(ROI), 1):\n",
        "  city_name = ROI[roi]['cityName']\n",
        "  year_begin = ROI[roi]['yearBegin']\n",
        "  year_end = ROI[roi]['yearEnd']\n",
        "\n",
        "  IMAGE_FILE_PREFIX = city_name + '_' + str(year_begin) + '_' + str(year_end)\n",
        "  FOLDER_NAME = USER_NAME + '_' + IMAGE_FILE_PREFIX + '_' + str(timestamp)\n",
        "  FOLDER_PATH = MY_DRIVE_PATH + '/' + FOLDER_NAME\n",
        "  TRAIN_FILE_PREFIX = 'Training_' + str(timestamp)\n",
        "  TEST_FILE_PREFIX = 'Testing_' + str(timestamp)\n",
        "\n",
        "  file_extension = '.tfrecord.gz'\n",
        "  TRAIN_FILE_PATH =  MY_DRIVE_PATH + '/' + FOLDER_NAME + '/sample/' +TRAIN_FILE_PREFIX + file_extension\n",
        "  TEST_FILE_PATH = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/sample/' + TEST_FILE_PREFIX + file_extension\n",
        "\n",
        "  OUTPUT_IMAGE_TFR = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/'+ IMAGE_FILE_PREFIX+'.TFRecord'\n",
        "  OUTPUT_IMAGE_TIF = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/'+ IMAGE_FILE_PREFIX+'.tif'\n",
        "\n",
        "  for year in range(year_begin, year_end+1, 1):\n",
        "    nSampleClass = []\n",
        "    out_ext = ee.Geometry.Rectangle([ ROI[roi]['LonMin'], ROI[roi]['LatMin'], ROI[roi]['LonMax'], ROI[roi]['LatMax']])\n",
        "    EXPORT_REGION = out_ext\n",
        "    out_ext_center = out_ext.centroid()\n",
        "\n",
        "    out_ext_buffer = out_ext.buffer(buffer_distance).bounds()\n",
        "\n",
        "    center_lat = out_ext_center.getInfo()['coordinates'][1]\n",
        "    center_lon = out_ext_center.getInfo()['coordinates'][0]\n",
        "\n",
        "    landsat_params = select_landsat(year_begin)\n",
        "\n",
        "    gtYearBegin = year_begin\n",
        "    gtYearEnd = year_end\n",
        "\n",
        "    product = landsat_product(landsat_params)\n",
        "\n",
        "    ref_point = ee.FeatureCollection(GT_DATA).map(reduce_class_value)\n",
        "    LABEL_DATA = ref_point.filter(ee.Filter.And(ee.Filter.greaterThanOrEquals('year', gtYearBegin), ee.Filter.lessThanOrEquals('year', gtYearEnd), ROI[roi]['doyFilter']))\n",
        "    LABEL_DATA = LABEL_DATA.filterBounds(product['region'])\n",
        "    if (year_end > 1982):\n",
        "      add_indices(landsat_params, product)\n",
        "    else:\n",
        "      add_indices_mss(landsat_params, product)\n",
        "\n",
        "    gt_image = product['image']\n",
        "    gt_image_texture = product['image_int']\n",
        "    for i in kernel_size_list:\n",
        "      gt_image_texture = gt_image.addBands(product['image_int'].select(landsat_params['TextureBand']).entropy(ee.Kernel.gaussian(i)))\n",
        "\n",
        "\n",
        "    out_image = product['image'].clip(out_ext_buffer)\n",
        "    out_image_texture = product['image_int'].clip(out_ext_buffer)\n",
        "    for i in kernel_size_list:\n",
        "      out_image_texture = out_image.addBands(product['image_int'].clip(out_ext_buffer).select(landsat_params['TextureBand']).entropy(ee.Kernel.gaussian(i)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gt_image.getInfo())"
      ],
      "metadata": {
        "id": "sEufutc9D0CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keGx5ESeZ4cS"
      },
      "source": [
        "BANDS = landsat_params['bandsClassify']\n",
        "LABEL = 'class'\n",
        "N_CLASSES = 4\n",
        "\n",
        "FEATURE_NAMES = list(BANDS)\n",
        "FEATURE_NAMES.append(LABEL)\n",
        "\n",
        "sample = gt_image_texture.sampleRegions(\n",
        "    collection = LABEL_DATA,\n",
        "    properties = [LABEL],\n",
        "    scale = spatial_resolution,\n",
        ").randomColumn()\n",
        "\n",
        "training = sample.filter(ee.Filter.lt('random', 0.8))\n",
        "testing = sample.filter(ee.Filter.gte('random', 0.8))\n",
        "\n",
        "#pprint({'training': training.first().getInfo()})\n",
        "#pprint({'testing': testing.first().getInfo()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAYHijezmYjl"
      },
      "source": [
        "training_task = ee.batch.Export.table.toDrive(\n",
        "  collection=training,\n",
        "  description='Training Export',\n",
        "  folder=FOLDER_NAME,\n",
        "  fileNamePrefix=TRAIN_FILE_PREFIX,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=FEATURE_NAMES)\n",
        "\n",
        "testing_task = ee.batch.Export.table.toDrive(\n",
        "  collection=testing,\n",
        "  description='Testing Export',\n",
        "  folder=FOLDER_NAME,\n",
        "  fileNamePrefix=TEST_FILE_PREFIX,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=FEATURE_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWURzdwvm_kC"
      },
      "source": [
        "training_task.start()\n",
        "testing_task.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgVIojLinCGS"
      },
      "source": [
        "while training_task.active():\n",
        "  print('Polling for task (id: {}).'.format(training_task.id))\n",
        "  time.sleep(20)\n",
        "print('Done with training export.')\n",
        "\n",
        "while testing_task.active():\n",
        "  print('Polling for task (id: {}).'.format(testing_task.id))\n",
        "  time.sleep(20)\n",
        "print('Done with testing export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgG86HDPuU9p"
      },
      "source": [
        "time.sleep(20)\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZnC6g5twMm5"
      },
      "source": [
        "src_dir = MY_DRIVE_PATH + '/' + FOLDER_NAME\n",
        "src_files = os.listdir(src_dir)\n",
        "\n",
        "dest_dir = os.path.join(src_dir, 'sample')\n",
        "\n",
        "os.makedirs(dest_dir, exist_ok = True)\n",
        "\n",
        "for file in src_files:\n",
        "    shutil.move(os.path.join(src_dir, file), dest_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsS6YBcjnSJ7"
      },
      "source": [
        "print('Found training file.' if tf.io.gfile.exists(TRAIN_FILE_PATH) \n",
        "    else 'No training file found.')\n",
        "\n",
        "print('Found testing file.' if tf.io.gfile.exists(TEST_FILE_PATH) \n",
        "    else 'No testing file found.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN-cQlaS0yEP"
      },
      "source": [
        "train_dataset = tf.data.TFRecordDataset(TRAIN_FILE_PATH, compression_type='GZIP')\n",
        "test_dataset = tf.data.TFRecordDataset(TEST_FILE_PATH, compression_type='GZIP')\n",
        "\n",
        "# print(iter(train_dataset).next())\n",
        "# print(iter(test_dataset).next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJiRlwiq3q4O"
      },
      "source": [
        "columns = [\n",
        "           tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
        "]\n",
        "\n",
        "features_dict = dict(zip(FEATURE_NAMES, columns))\n",
        "\n",
        "pprint(features_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnes6DPZTs8V"
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "  labels = parsed_features.pop(LABEL)\n",
        "  return parsed_features, tf.cast(labels, tf.int32)\n",
        "\n",
        "parsed_trainset = train_dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "parsed_testset = test_dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "\n",
        "# pprint(iter(parsed_trainset))\n",
        "# pprint(iter(parsed_testset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tcHrlkuU1_S"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "def to_tuple(inputs, label):\n",
        "  return (tf.transpose(list(inputs.values())), tf.one_hot(indices=label, depth=N_CLASSES))\n",
        "\n",
        "input_dataset = parsed_trainset.map(to_tuple).batch(batch_size)\n",
        "validate_dataset = parsed_testset.map(to_tuple).batch(batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "694bORamYglL"
      },
      "source": [
        "drop = 0.05\n",
        "N_UNITS=64\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(N_UNITS, activation='relu'))\n",
        "for i in range(8):\n",
        "  model.add(tf.keras.layers.Dropout(drop))\n",
        "  model.add(tf.keras.layers.Dense(N_UNITS, activation='relu'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(N_CLASSES, activation=tf.nn.softmax))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS81n0jVZKbU"
      },
      "source": [
        "from keras_radam import RAdam\n",
        "\n",
        "model.compile(\n",
        "    optimizer=RAdam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ6GHY6qZOjt"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "log_d = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/' + 'model'\n",
        "os.makedirs(log_d, exist_ok=True)\n",
        "\n",
        "def build_callbacks():\n",
        "    checkpointer = ModelCheckpoint(filepath = log_d + '/best_model.h5', verbose=0, save_best_only=True, monitor='val_loss')\n",
        "    callbacks = [checkpointer]\n",
        "    return callbacks\n",
        "history = model.fit(x=input_dataset,validation_data=(validate_dataset), epochs=100, callbacks=build_callbacks())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD3oemUAgGcM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNb394HLdF1c"
      },
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model(log_d + '/best_model.h5', custom_objects={'RAdam': RAdam})\n",
        "model.evaluate(validate_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCfGLPzQhKE1"
      },
      "source": [
        "image_export_options = {\n",
        "    'patchDimensions': [patch_size, patch_size],\n",
        "    'maxFileSize': 104857600,\n",
        "    'compressed': True\n",
        "}\n",
        "\n",
        "image_task = ee.batch.Export.image.toDrive(\n",
        "  image=out_image_texture,\n",
        "  description='Image Export',\n",
        "  fileNamePrefix= IMAGE_FILE_PREFIX,\n",
        "  folder= FOLDER_NAME,\n",
        "  scale=spatial_resolution,\n",
        "  fileFormat='TFRecord',\n",
        "  region=out_ext_buffer,\n",
        "  formatOptions=image_export_options,\n",
        ")\n",
        "\n",
        "image_task.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7UiBAvvjnlF"
      },
      "source": [
        "while image_task.active():\n",
        "  print('Polling for task (id: {}).'.format(image_task.id))\n",
        "  time.sleep(20)\n",
        "print('Done with image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQfKDvknu6Fq"
      },
      "source": [
        "time.sleep(20)\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGbbvGEyj59c"
      },
      "source": [
        "tfrecord_list = glob.glob(MY_DRIVE_PATH +'/' + FOLDER_NAME + '/' + '*.tfrecord.gz')\n",
        "tfrecord_list.sort()\n",
        "print(tfrecord_list)\n",
        "\n",
        "json_list = glob.glob(MY_DRIVE_PATH +'/' + FOLDER_NAME + '/' + '*.json')\n",
        "print(json_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1rNwK32zyR"
      },
      "source": [
        "with open(json_list[0], \"r\") as read_file:\n",
        "   mixer = json.load(read_file)\n",
        "\n",
        "mixer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNi0Q7yF4vMo"
      },
      "source": [
        "patch_width = mixer['patchDimensions'][0]\n",
        "patch_height = mixer['patchDimensions'][1]\n",
        "patches = mixer['totalPatches']\n",
        "patch_dimensions_flat = [patch_width*patch_height, 1]\n",
        "\n",
        "image_columns = [\n",
        "                tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) for k in BANDS\n",
        "]\n",
        "\n",
        "image_features_dict = dict(zip(BANDS, image_columns))\n",
        "\n",
        "image_dataset = tf.data.TFRecordDataset(tfrecord_list, compression_type='GZIP')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N5glJOH6n6r"
      },
      "source": [
        "def parse_image(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, image_features_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLzPjxt469t2"
      },
      "source": [
        "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
        "\n",
        "image_dataset = image_dataset.flat_map(\n",
        "    lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
        ")\n",
        "\n",
        "image_dataset = image_dataset.map(\n",
        "  lambda data_dict: (tf.transpose(list(data_dict.values())), )\n",
        ")\n",
        "\n",
        "image_dataset = image_dataset.batch(patch_width * patch_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrb_7arg7dHU"
      },
      "source": [
        "predictions = model.predict(image_dataset, steps=patches, verbose=1)\n",
        "print(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZ-xaCa7wth"
      },
      "source": [
        "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_TFR)\n",
        "\n",
        "patch = [[]]\n",
        "cur_patch = 1\n",
        "for prediction in predictions:\n",
        "  patch[0].append(tf.argmax(prediction, 1))\n",
        "\n",
        "  if (len(patch[0])==patch_width*patch_height):\n",
        "    print('Done with Patch ' + str(cur_patch) + ' of ' + str(patches) + '...')\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "                'prediction': tf.train.Feature(\n",
        "                    int64_list=tf.train.Int64List(\n",
        "                        value=patch[0]\n",
        "                    )\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    writer.write(example.SerializeToString())\n",
        "    patch=[[]]\n",
        "    cur_patch +=1\n",
        "  \n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgMfcE4qBdIR"
      },
      "source": [
        "record_iterator = tf.compat.v1.python_io.tf_record_iterator(path=OUTPUT_IMAGE_TFR)\n",
        "\n",
        "n_row = patches/mixer['patchesPerRow']\n",
        "n_col = mixer['patchesPerRow']\n",
        "current_row = 0\n",
        "current_col = 0\n",
        "counter = 0\n",
        "\n",
        "for string_record in record_iterator:\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(string_record)\n",
        "  values = np.array(example.features.feature['prediction'].int64_list.value).reshape(patch_width, patch_height).astype(np.int8)\n",
        "\n",
        "  if (current_col==0):\n",
        "    horizontal_strip = values\n",
        "  else:\n",
        "    horizontal_strip = np.concatenate([horizontal_strip, values], axis=1)\n",
        "  current_col += 1\n",
        "\n",
        "  if (current_col == n_col):\n",
        "    if (current_row==0):\n",
        "      image = horizontal_strip\n",
        "    else:\n",
        "      image = np.concatenate([image, horizontal_strip], axis=0)\n",
        "      horizontal_strip = []\n",
        "\n",
        "    current_row +=1\n",
        "    current_col = 0\n",
        "    print(counter)\n",
        "  counter +=1\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNbWNYBIEMaX"
      },
      "source": [
        "imgplot = plt.imshow(image, cmap=plt.get_cmap('jet'), vmin=0, vmax=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nzvw12lEXc8"
      },
      "source": [
        "affine = mixer['projection']['affine']['doubleMatrix']\n",
        "geotransform = (affine[2], affine[0], affine[1], affine[5], affine[3], affine[4])\n",
        "crs = int(mixer['projection']['crs'][-4:])\n",
        "print(affine)\n",
        "print(geotransform)\n",
        "print(crs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAD6NJyFzdP"
      },
      "source": [
        "ny, nx = image.shape\n",
        "dst_ds = gdal.GetDriverByName('GTiff').Create(OUTPUT_IMAGE_TIF, nx, ny, 1, gdal.GDT_Byte)\n",
        "dst_ds.SetGeoTransform(tuple(geotransform))\n",
        "srs = osr.SpatialReference()\n",
        "srs.ImportFromEPSG(crs)\n",
        "dst_ds.SetProjection(srs.ExportToWkt())\n",
        "dst_ds.GetRasterBand(1).WriteArray(image)\n",
        "dst_ds.FlushCache()\n",
        "dst_ds = None\n",
        "\n",
        "print(\"Exorting \" + OUTPUT_IMAGE_TIF + \" completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1eCb4yIlZcR"
      },
      "source": [
        "fcc_image = out_image.select(landsat_params['FCCbands'])\n",
        "fcc_prefix = city_name + '_' + str(year_begin) + '_' + str(year_end)+'_fcc'\n",
        "\n",
        "fcc_task = ee.batch.Export.image.toDrive(**{\n",
        "    'image': fcc_image,\n",
        "    'description': fcc_prefix,\n",
        "    'folder': FOLDER_NAME,\n",
        "    'fileNamePrefix': fcc_prefix,\n",
        "    'scale': spatial_resolution,\n",
        "    'region': out_ext\n",
        "})\n",
        "fcc_task.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7jLFSe-nYHz"
      },
      "source": [
        "while fcc_task.active():\n",
        "  print('Polling for task (id: {}).'.format(fcc_task.id))\n",
        "  time.sleep(20)\n",
        "print('Done with fcc image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi_6iYZeHclU"
      },
      "source": [
        "time.sleep(20)\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZbpkBfh2rZ9"
      },
      "source": [
        "lc_dir = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/lc_map'\n",
        "fcc_path = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/'+ fcc_prefix+'.tif'\n",
        "ref_dir = MY_DRIVE_PATH + '/' + FOLDER_NAME + '/ref_map'\n",
        "\n",
        "os.makedirs(lc_dir, exist_ok = True)\n",
        "os.makedirs(ref_dir, exist_ok = True)\n",
        "\n",
        "shutil.move(OUTPUT_IMAGE_TIF, lc_dir)\n",
        "shutil.move(OUTPUT_IMAGE_TFR, lc_dir)\n",
        "shutil.move(fcc_path, ref_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5kyPo5tXKiC"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.move(FOLDER_PATH, SHARED_DRIVE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}